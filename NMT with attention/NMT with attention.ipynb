{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci9UU0GCfIPY"
   },
   "source": [
    "## Connecting to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3vQxRw8513_",
    "outputId": "2acdad05-1813-4d53-e842-d1383012fb8d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/Github/Natural-Language-Processing/NMT with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMpyLfZrfD16"
   },
   "source": [
    "## Installing trax library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iByuGNDw5gqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MmOEpFwd5V2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                     1.3.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH0nFdxQ3zyF"
   },
   "source": [
    "# Data Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLPyOL3UhMrv"
   },
   "source": [
    "## Loading dataset\n",
    "This dataset contains a subset of (english, german) sentences pairs of medical related texts collected from opus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c-tAr4g65ZYl"
   },
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified.\n",
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),  #eenglish and german sentences pair\n",
    "                                 eval_holdout_size=0.01, # 1% for eval\n",
    "                                 train=True)\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                data_dir='./data/',\n",
    "                                keys=('en', 'de'),\n",
    "                                eval_holdout_size=0.01, # 1% for eval\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udUNOrVC6J0_",
    "outputId": "192aaea3-3778-485d-8a92-d14a56ad3d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data (en, de) tuple: (b'In the pregnant rat the AUC for calculated free drug at this dose was approximately 18 times the human AUC at a 20 mg dose.\\n', b'Bei tr\\xc3\\xa4chtigen Ratten war die AUC f\\xc3\\xbcr die berechnete ungebundene Substanz bei dieser Dosis etwa 18-mal h\\xc3\\xb6her als die AUC beim Menschen bei einer 20 mg Dosis.\\n')\n",
      "eval data (en, de) tuple: (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "eval_stream = eval_stream_fn()\n",
    "#inside the tuple sentences are stored in bytes format\n",
    "print('train data (en, de) tuple:', next(train_stream))\n",
    "print('eval data (en, de) tuple:', next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTAU1vB7fwjK"
   },
   "source": [
    "## Tokenizing datase\n",
    "Tokenizing involves splitting sentences into words and then converting words to integers using vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nflh3dw-CtGN"
   },
   "outputs": [],
   "source": [
    "VOCAB_FILE = 'ende_32k.subword' #ende_32k.subword is a vocabulary containing both english and german subwords. (hint: byte pair encoding)\n",
    "VOCAB_DIR = 'data/'\n",
    "# Tokenizing the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)\n",
    "\n",
    "#adding EOS at the end of each sentence\n",
    "EOS = 1 #since it is at index 1 in vocab\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va-Jcoz-gK2X"
   },
   "source": [
    "## Filtering out too long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DQrQXHn1Jzsu"
   },
   "outputs": [],
   "source": [
    "# Filtering too long sentences to not run out of memory.\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256, length_keys=[0, 1])(tokenized_train_stream) #length_keys=[0, 1] specifies both inputs and targets in the tuple\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FbGZidTgXtn"
   },
   "source": [
    "## building tokenize() and detokenize() functions for individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XaRrIDatK4ZN"
   },
   "outputs": [],
   "source": [
    "# Setup helper functions for tokenizing and detokenizing sentences\n",
    "def tokenize(sentence, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    # trax.data.tokenize takes data generator as input. to convert a sentence into generator we can use iter([sentence])\n",
    "    inputs =  next(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    # Adding the batch dimension to the front of the shape\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    return batch_inputs\n",
    "\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "    EOS = 1\n",
    "    # Removing the EOS to decode only the original tokens\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)] \n",
    "    sentence = trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir) \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5tgEVI_LP7i",
    "outputId": "365b096a-347f-48d7-96a6-aef7bd9a8eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize('hello'):  [[17332   140     1]]\n",
      "detokenize([17332, 140, 1]):  hello\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords 'hell' and 'o' to form the word 'hello'.\n",
    "print(f\"tokenize('hello'): \", tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(f\"detokenize([17332, 140, 1]): \", detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft7KNV_dhYw7"
   },
   "source": [
    "## Bucketing\n",
    "Bucketing places similar sized sentences to the same batch. By bucketing we need minimal padding to make equal length sentences in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjR9z8Z6jmJ8",
    "outputId": "760917ed-31e1-4b89-b944-6fef43b993e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Bucketing to create streams of batches. Buckets are defined in terms of boundaries and batch sizes.\n",
    "   Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "   So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
    "   between 8 and 16, and so on -- and only 2 if length > 512.'''\n",
    "\n",
    "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
    "# Create the generators.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "      boundaries, batch_sizes,\n",
    "      length_keys=[0, 1]  # count inputs and targets to length.\n",
    "      )(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "      boundaries, batch_sizes,\n",
    "      length_keys=[0, 1]  # count inputs and targets to length.\n",
    "      )(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s). <pad> token is at index 0 in vocab file\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)\n",
    "\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZyQc65Unk7s",
    "outputId": "b5bcf54a-ed7f-4532-f2e7-f53fcb6ed7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence: The adjusted mean difference was -4.3 points (CI 95% -6.4; -2.1 points, p-value < 0.0001).\n",
      " \n",
      "\n",
      "Tokenized version of english sentence: \n",
      " [   29  9701  1516  2640    53  1581   219     3   199  1164    50  7082\n",
      "     5  4207 11767    15   330     3   219  7108    15   150     3   135\n",
      "  1164     2   719    15   980   909 33287   913   266     3  8074  3912\n",
      " 33022 30650  4729   992     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "German sentence:  Die angepasste mittlere Differenz betrug -4,3 Punkte (95 %-Konfidenzintervall: -6,4 bis -2,1 Punkte, p-Wert < 0,0001).\n",
      " \n",
      "\n",
      "Tokenized version of german sentence: \n",
      " [   57 30482  8385   191 14998     5 12919 20657  1581   219   227   199\n",
      "  2927    50  4207 11770    15 11580  7770 13427  9436 19070     5  2801\n",
      "    15   330   227   219   248  1581   150   227   135  2927     2   719\n",
      "    15  1619   909 33287   913   266   227  8074  3912 33022 30650  4729\n",
      "   992     1     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "print('English sentence:', detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print('Tokenized version of english sentence: \\n', input_batch[index], '\\n')\n",
    "print('German sentence: ', detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print('Tokenized version of german sentence: \\n', target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhPgwKEI3p-M"
   },
   "source": [
    "# Model Section\n",
    "\n",
    "<img src = \"NMT_Model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JO9yt2hR3-cu"
   },
   "source": [
    "## Function to build Encoder\n",
    "This is a helper function that stacks trax layers to build encoder portion for the NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UZ1ztqdYpCWw"
   },
   "outputs": [],
   "source": [
    "def input_encoder_fn(input_vocab_size, d_model, num_encoder_layers):  # d_model: depth of embedding (n_units in the LSTM cell)    \n",
    "    input_encoder = tl.Serial( \n",
    "        tl.Embedding(vocab_size=input_vocab_size, d_feature=d_model),\n",
    "        [tl.LSTM(n_units=d_model) for _ in range(num_encoder_layers)] #LSTM cell returns full sequence as output\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWgdEMZ5gLr"
   },
   "source": [
    "## Function to build pre-attention decoder\n",
    "Pre-attention decoder runs on the targets and creates activations that are used as queries in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pZbeJgo35lBi"
   },
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model): # mode: str: 'train' or 'eval'\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        # shift right to insert start-of-sentence token and implement teacher forcing during training\n",
    "        # in teacher forcing, actual previous target is used to predict next token instead of decoder preivous output \n",
    "        tl.ShiftRight(mode=mode), # It does nothing if mode = \"eval\"\n",
    "        tl.Embedding(vocab_size=target_vocab_size, d_feature=d_model),\n",
    "        tl.LSTM(n_units=d_model) #LSTM cell returns full sequence as output\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NERnGWm6Hrg"
   },
   "source": [
    "## Function to prepare Queries, Keys and Values for Attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X50m9uPq6UEs"
   },
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
    "    \"\"\"\n",
    "    # seting the keys and values to the encoder activations\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "    # setting the queries to the decoder activations\n",
    "    queries = decoder_activations\n",
    "    # generating the mask to distinguish real tokens from padding\n",
    "    mask = inputs != 0\n",
    "    # adding axes to the mask for attention heads and decoder length.\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    # broadcasting so that mask shape is (batch_size, attention_heads, decoder_len, encoder_len). Here attention_heads = 1.\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1)) #mask: (batch_size, attention_heads, decoder_len, encoder_len)\n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDxtEKIe7bqS"
   },
   "source": [
    "## Neural Machine Translation(NMT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NtBBi3CV7hz6"
   },
   "outputs": [],
   "source": [
    "# See NMT_Model.png to understand architecture\n",
    "def NMT_Model(input_vocab_size=33300, target_vocab_size=33300, d_model=1024, num_encoder_layers=2,\n",
    "            num_decoder_layers=2, num_attention_heads=4, attention_dropout=0.0, mode='train'):  #mode (str): 'train', 'eval' or 'predict'\n",
    "\n",
    "    input_encoder = input_encoder_fn(input_vocab_size, d_model, num_encoder_layers)\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "    model = tl.Serial( \n",
    "      # copying input tokens and target tokens as they will be needed later.\n",
    "      tl.Select([0, 1, 0, 1]),   #signal stack becomes = [input_tokens, target_tokens, input_tokens, target_tokens]\n",
    "      # running input encoder on the input and pre-attention decoder on the target.\n",
    "      tl.Parallel(input_encoder, pre_attention_decoder),   #signal stack becomes = [input_tokens, target_tokens]\n",
    "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "      # Nest Attention layer inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
    "      # The Residual layer will accept a layer as an argument and it will add the output of that layer to the current stack top input.\n",
    "      tl.Residual(tl.AttentionQKV(d_model, n_heads=num_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "      # drop attention mask. signal stack currently has [attention_activations, mask, target_tokens]\n",
    "      tl.Select([0,2]),\n",
    "      # RNN decoder\n",
    "      [tl.LSTM(n_units=d_model) for _ in range(num_decoder_layers)],\n",
    "      tl.Dense(target_vocab_size),\n",
    "      tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jd0dN-3oIGaC",
    "outputId": "936bb668-fa62-4e18-8c0a-0d27477e2e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      ShiftRight(1)\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        Parallel_in3_out3[\n",
      "          Dense_1024\n",
      "          Dense_1024\n",
      "          Dense_1024\n",
      "        ]\n",
      "        PureAttention_in4_out2\n",
      "        Dense_1024\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NMT_Model(mode='train')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pCHf34lIPDe"
   },
   "source": [
    "## Define train task and eval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2-urfu0Eqpj",
    "outputId": "dff72d28-7a1e-49d6-f8d2-c0a3db551680"
   },
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    labeled_data = train_batch_stream,\n",
    "    loss_layer = tl.CrossEntropyLoss(),\n",
    "    optimizer = trax.optimizers.Adam(0.01),\n",
    "    # Using learning rate scheduler to have 1000 warmup steps with a max value of 0.01\n",
    "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
    "    n_steps_per_checkpoint= 10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data = eval_batch_stream,\n",
    "    metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")\n",
    "\n",
    "\n",
    "output_dir = 'model/'\n",
    "# remove old model if it exists.\n",
    "!rm -f ~/output_dir/model.pkl.gz  \n",
    "\n",
    "training_loop = training.Loop(model, train_task, eval_tasks=[eval_task], output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIS49FPLIV58"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p52-aJLXZRyU",
    "outputId": "80690317-e2b8-4507-d14f-c22783e8da9b"
   },
   "outputs": [],
   "source": [
    "#training_loop.run(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHKPC6h4ZVl3"
   },
   "source": [
    "## Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fRVn_BvWY8T3"
   },
   "outputs": [],
   "source": [
    "model = NMT_Model(mode='eval')\n",
    "# initialize weights from a pre-trained model\n",
    "model.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmgyZHtiZqvQ"
   },
   "source": [
    "## Producing output sentence from decoder output representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEKcTNilaSXe"
   },
   "source": [
    "## Function to get index of next token\n",
    "This function takes input sentence tokens and current decoded words tokens(0 initially) as input and returns the index of next word (selected from decoder generated log probabilities). Trax has built in layer named  logsoftmax_sample to select a word from log propabilities. temperature parameter is used to control noise added to log probabilities inside logsoftmax_sample layer.                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oAx1fjKmZ8bw"
   },
   "outputs": [],
   "source": [
    "def next_symbol(NMT_Model, input_tokens, cur_output_tokens, temperature):\n",
    "    output_tokens_len = len(cur_output_tokens)\n",
    "    # calculating next power of 2 for padding length. For example if len=13 then next power of 2 will be 16. \n",
    "    padded_length = np.power(2, int(np.ceil(np.log2(output_tokens_len + 1)))) #We add 1 to avoid log(0).\n",
    "    padded = cur_output_tokens + [0] * (padded_length - output_tokens_len)\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0) # adding batch dimention\n",
    "    # model outputs:  [log probabilities, target tokens]\n",
    "    output, _ = NMT_Model((input_tokens, padded_with_batch)) #output: (batch_size, decoder_length, vocab_size)\n",
    "    # getting log probabilities from the last token output\n",
    "    log_probs = output[0, output_tokens_len, :]\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    return symbol, float(log_probs[symbol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baZKqCsEtN6a"
   },
   "source": [
    "## Function to generate final translated sentence\n",
    "This will call the next_symbol() function above several times until the next output is the end-of-sentence token (i.e. EOS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CyrBMux-tPfJ"
   },
   "outputs": [],
   "source": [
    "def sampling_decode(input_sentence, NMT_Model = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
    "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
    "    cur_output_tokens = []\n",
    "    cur_output = 0\n",
    "    EOS = 1\n",
    "    while cur_output != EOS:\n",
    "        # updating the current output token by getting the index of the next word\n",
    "        cur_output, log_prob = next_symbol(NMT_Model, input_tokens, cur_output_tokens, temperature)\n",
    "        cur_output_tokens.append(cur_output)\n",
    "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir) \n",
    "\n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OeXfV_Venmqf",
    "outputId": "b18da8a6-4430-4ee3-a709-43728dfa12ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([161, 12202, 5112, 3, 1], -0.0001735687255859375, 'Ich liebe Sprachen.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSLuPBvIqZWD"
   },
   "source": [
    "## Minimum Bayes-Risk Decoding (MBR)\n",
    "MBR works as following:\n",
    "1. take several random samples\n",
    "2. score each sample against all other samples\n",
    "3. select the one with the highest score<br>\n",
    "<b>Scoring is done by computing overlap between a pair of samples. Here we consider only unigram overlap.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cROUBso5qEFu"
   },
   "outputs": [],
   "source": [
    "def generate_samples(sentence, num_samples, NMT_Model=None, temperature=1, vocab_file=None, vocab_dir=None):\n",
    "    samples, log_probs = [], []\n",
    "    for _ in range(num_samples):\n",
    "        sample_tokens, logp, _ = sampling_decode(sentence, NMT_Model, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "        samples.append(sample_tokens)\n",
    "        log_probs.append(logp)\n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dJb-TggRqnI4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[161, 12202, 5112, 3, 1],\n",
       "  [161, 12202, 10, 5112, 3, 1],\n",
       "  [161, 12202, 5112, 3, 1],\n",
       "  [161, 12202, 5112, 3, 1]],\n",
       " [-0.0001735687255859375,\n",
       "  -0.0001087188720703125,\n",
       "  -0.0001735687255859375,\n",
       "  -0.0001735687255859375])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bFZcai5rFcv"
   },
   "source": [
    "## Comparing overlaps\n",
    "One of the more simple metrics is the Jaccard similarity which gets the intersection over union of two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ajs75htXrGxy"
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
    "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
    "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
    "    overlap = len(joint_elems) / len(all_elems)\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36l9Fxhrt2uQ"
   },
   "source": [
    "## Calculating ROUGE score\n",
    "ROUGE score (similar to f1 score) also calculates overlapping between two samples. When ROUGE score is calculated on unigrams then it is called ROUGE-1\n",
    "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FbTvH8xmt5dr"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def rouge1_similarity(system, reference):\n",
    "    # making a frequency tables of the system and reference tokens\n",
    "    sys_counter = Counter(system)\n",
    "    ref_counter = Counter(reference)\n",
    "    overlap = 0\n",
    "    for token in sys_counter:\n",
    "        token_count_sys = sys_counter.get(token, 0) # token frequency in system translation        \n",
    "        token_count_ref = ref_counter.get(token, 0) # token frequency in reference\n",
    "        # updating the overlap by getting the smaller number between the two token counts above\n",
    "        overlap += min(token_count_sys, token_count_ref)\n",
    "    precision = overlap / sum(sys_counter.values())  #by defination\n",
    "    recall = overlap / sum(ref_counter.values())  #by defination\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    else:\n",
    "        rouge1_score = 0     \n",
    "    return rouge1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlU0qH_SwdMM"
   },
   "source": [
    "## Overall score\n",
    "Consider 4-sample list.\n",
    "\n",
    "1. Get similarity score between sample 1 and sample 2\n",
    "2. Get similarity score between sample 1 and sample 3\n",
    "3. Get similarity score between sample 1 and sample 4\n",
    "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
    "5. Iterate and repeat until samples 1 to 4 have overall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dtf2X88lwl7i"
   },
   "outputs": [],
   "source": [
    "def average_overlap(similarity_fn, samples, *ignore):\n",
    "    scores = {}\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        overlap = 0.0\n",
    "        cnt = 0\n",
    "        for index_sample, sample in enumerate(samples): \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            cnt += 1\n",
    "            overlap += sample_overlap\n",
    "        score = overlap/cnt\n",
    "        scores[index_candidate] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeHlUwlbx7bi"
   },
   "source": [
    "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_q-2KZNpx7-f"
   },
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        log_probs (list of float): log probability of the translated sentences\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "            # converting log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "            weight_sum += sample_p\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            overlap += sample_p * sample_overlap\n",
    "        score = overlap / weight_sum\n",
    "        scores[index_candidate] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiWCYLCvy38L"
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "BDL8oo5_y5md"
   },
   "outputs": [],
   "source": [
    "def mbr_decode(sentence, num_samples, score_fn, similarity_fn, NMT_Model=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    samples, log_probs = generate_samples(sentence, num_samples, NMT_Model, temperature, vocab_file, vocab_dir)\n",
    "    scores = score_fn(similarity_fn, samples, log_probs)\n",
    "    max_index = max(scores, key=scores.get)\n",
    "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
    "    return (translated_sentence, max_index, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IrunYsYJ6EuS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sie spricht Englisch und Deutsch.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEMPERATURE = 0.6\n",
    "sen = 'She speaks English and German.'\n",
    "mbr_decode(sen, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I-rthf_Y6MEZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Herzlichen Gl√ºckwunsch!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = 'Congratulations!'\n",
    "mbr_decode(sen, 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
