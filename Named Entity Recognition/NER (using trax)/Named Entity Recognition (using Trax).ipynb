{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Named Entity Recognition (using Trax).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOh/8GXL3/hATx+gKtChnC5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sxOm0EEO-VgK"},"source":["##Connection to Google Drive"]},{"cell_type":"code","metadata":{"id":"sCHc8GiewOEP"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ml0p69OwwQmF"},"source":["%cd drive/MyDrive/Colab Notebooks/Temp/NER"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QP34KyH-iBD"},"source":["##Importing necessary items"]},{"cell_type":"code","metadata":{"id":"LWXCSM0TV5b5"},"source":["!pip install trax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8T1PrTlxbMn"},"source":["import os \n","import trax \n","from trax import layers as tl\n","import numpy as np\n","import pandas as pd\n","import random as rnd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IgfqUhr7-rHx"},"source":["##Loading and Preprocessing Dataset\n"]},{"cell_type":"code","metadata":{"id":"niHPlNnyuVz2"},"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    idx2word = []\n","    with open(vocab_path) as f:\n","        for i, w in enumerate(f.read().splitlines()):\n","            vocab[w] = i\n","            idx2word.append(w)\n","    vocab['<PAD>'] = len(vocab)\n","    vocab['<UNK>'] = len(vocab)\n","    idx2word.append('<PAD>')\n","    idx2word.append('<UNK') \n","\n","    tag_map = {}\n","    idx2tag = []\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","            idx2tag.append(t) \n","\n","    return vocab, idx2word, tag_map, idx2tag\n","\n","\n","\n","\n","def preprocess(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK WORD\n","            s = [vocab[token] if token in vocab \n","                 else vocab['<UNK>']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            l = [tag_map[label] for label in sentence.split(' ')] \n","            labels.append(l) \n","    return sentences, labels, len(sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9ZvIsiXwlpF"},"source":["vocab, idx2word, tag_map, idx2tag = get_vocab('data/words.txt', 'data/tags.txt')\n","train_sen, train_labels, train_size = preprocess(vocab, tag_map, 'data/train/sentences.txt', 'data/train/labels.txt')\n","valid_sen, valid_labels, valid_size = preprocess(vocab, tag_map, 'data/val/sentences.txt', 'data//val/labels.txt')\n","test_sen, test_labels, test_size = preprocess(vocab, tag_map, 'data/test/sentences.txt', 'data/test/labels.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ztERUblc_IGC"},"source":["##Batch Generator"]},{"cell_type":"code","metadata":{"id":"TD0EkmMUxZRc"},"source":["def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n","    '''\n","      Input: \n","        batch_size - integer describing the batch size\n","        x - list containing sentences where words are represented as integers\n","        y - list containing tags associated with the sentences\n","        shuffle - Shuffle the data order\n","        pad - an integer representing a pad character\n","        verbose - Print information during runtime\n","      Output:\n","        a tuple containing 2 elements:\n","        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n","        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n","    '''\n","    num_lines = len(x)\n","    lines_index = [*range(num_lines)]\n","    \n","    if shuffle:\n","        rnd.shuffle(lines_index)\n","\n","    index = 0 # tracks current location in x, y\n","    while True:\n","        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n","        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n","                \n","        max_len = 0\n","        for i in range(batch_size):\n","            if index >= num_lines:\n","                index = 0\n","                if shuffle:\n","                    rnd.shuffle(lines_index)\n","\n","            buffer_x[i] = x[lines_index[index]]\n","            buffer_y[i] = y[lines_index[index]]\n","          \n","            lenx = len(x[lines_index[index]]) \n","            if lenx > max_len:\n","                max_len = lenx \n","\n","            index += 1\n","\n","\n","        # creating X, Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n","        X = np.full((batch_size, max_len), pad)\n","        Y = np.full((batch_size, max_len), pad)\n","\n","        for i in range(batch_size):\n","            x_i = buffer_x[i]\n","            y_i = buffer_y[i]\n","\n","            for j in range(len(x_i)):\n","                X[i, j] = x_i[j]\n","                Y[i, j] = y_i[j]\n","        if verbose: print(\"index=\", index)\n","        yield((X, Y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K99go7KttYNp"},"source":["##Building the model"]},{"cell_type":"code","metadata":{"id":"dlfxgunC_LfK"},"source":["vocab_size = len(vocab)\n","embedding_dims = 64\n","num_tags = len(tag_map)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZnXDYSW9yPK"},"source":["def NER(vocab_size=35180, d_model=50, tags=tag_map):\n","\n","    model = tl.Serial(\n","      tl.Embedding(vocab_size, d_model), \n","      tl.LSTM(d_model), \n","      tl.LSTM(d_model),\n","      tl.Dense(len(tags)),\n","      tl.LogSoftmax()  \n","      )\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfYNW8UOyujm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631024460782,"user_tz":-360,"elapsed":452,"user":{"displayName":"Md. Kamrul Hasan Tuhin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGTfHDQAsNbeejgBYTiTFOcb_5rLG_RZrR72nE4g=s64","userId":"09842155178617471284"}},"outputId":"7d3fddef-0794-4757-8345-6a390ea71bf5"},"source":["model = NER(vocab_size=vocab_size, d_model=embedding_dims, tags=tag_map)\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Serial[\n","  Embedding_35180_64\n","  LSTM_64\n","  LSTM_64\n","  Dense_17\n","  LogSoftmax\n","]\n"]}]},{"cell_type":"markdown","metadata":{"id":"KiSN-Kmwtmmv"},"source":["##Getting training and validation batches"]},{"cell_type":"code","metadata":{"id":"UuHIHJ5gFGjU"},"source":["from trax.supervised import training\n","batch_size = 64\n","\n","# Create training data, mask pad id=35178 for training.\n","train_generator = trax.data.inputs.add_loss_weights(\n","    data_generator(batch_size, train_sen, train_labels, vocab['<PAD>'], True), id_to_mask=vocab['<PAD>'])\n","\n","# Create validation data, mask pad id=35178 for training.\n","eval_generator = trax.data.inputs.add_loss_weights(\n","    data_generator(batch_size, valid_sen, valid_labels, vocab['<PAD>'], True), id_to_mask=vocab['<PAD>'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PADRPNivOcL"},"source":["##Training"]},{"cell_type":"code","metadata":{"id":"kRYCVdc1kR86"},"source":["def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n","\n","    train_task = training.TrainTask(\n","      train_generator,\n","      loss_layer = tl.CrossEntropyLoss(),\n","      optimizer = trax.optimizers.Adam(0.01), \n","    )\n","\n","    eval_task = training.EvalTask(\n","      labeled_data = eval_generator, \n","      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n","      n_eval_batches = 10 \n","    )\n","\n","    training_loop = training.Loop(\n","        NER,\n","        train_task, \n","        eval_tasks=[eval_task], \n","        output_dir = output_dir) \n","    \n","    training_loop.run(n_steps = train_steps)\n","    return training_loop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cws8RMuKkS67","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631024796732,"user_tz":-360,"elapsed":315889,"user":{"displayName":"Md. Kamrul Hasan Tuhin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGTfHDQAsNbeejgBYTiTFOcb_5rLG_RZrR72nE4g=s64","userId":"09842155178617471284"}},"outputId":"f50ffd4a-14d3-40a1-e0f7-f30479e2bbf9"},"source":["train_steps = 1000\n","!rm -f 'model/model.pkl.gz' \n","training_loop = train_model(model, train_generator, eval_generator, train_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:378: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n","  \"jax.host_id has been renamed to jax.process_index. This alias \"\n","/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:391: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n","  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"]},{"output_type":"stream","name":"stdout","text":["\n","Step      1: Total number of trainable weights: 2318673\n","Step      1: Ran 1 train steps in 4.62 secs\n","Step      1: train CrossEntropyLoss |  3.25448155\n","Step      1: eval  CrossEntropyLoss |  1.34228257\n","Step      1: eval          Accuracy |  0.81889203\n","\n","Step    100: Ran 99 train steps in 111.02 secs\n","Step    100: train CrossEntropyLoss |  0.75717318\n","Step    100: eval  CrossEntropyLoss |  0.55147647\n","Step    100: eval          Accuracy |  0.85894021\n","\n","Step    200: Ran 100 train steps in 35.76 secs\n","Step    200: train CrossEntropyLoss |  0.35892054\n","Step    200: eval  CrossEntropyLoss |  0.22245454\n","Step    200: eval          Accuracy |  0.94019287\n","\n","Step    300: Ran 100 train steps in 22.69 secs\n","Step    300: train CrossEntropyLoss |  0.20599809\n","Step    300: eval  CrossEntropyLoss |  0.17200291\n","Step    300: eval          Accuracy |  0.94976841\n","\n","Step    400: Ran 100 train steps in 22.55 secs\n","Step    400: train CrossEntropyLoss |  0.16318233\n","Step    400: eval  CrossEntropyLoss |  0.13841280\n","Step    400: eval          Accuracy |  0.96056877\n","\n","Step    500: Ran 100 train steps in 10.85 secs\n","Step    500: train CrossEntropyLoss |  0.14791387\n","Step    500: eval  CrossEntropyLoss |  0.14402557\n","Step    500: eval          Accuracy |  0.95671765\n","\n","Step    600: Ran 100 train steps in 14.81 secs\n","Step    600: train CrossEntropyLoss |  0.12144500\n","Step    600: eval  CrossEntropyLoss |  0.14670588\n","Step    600: eval          Accuracy |  0.95562445\n","\n","Step    700: Ran 100 train steps in 10.81 secs\n","Step    700: train CrossEntropyLoss |  0.11960235\n","Step    700: eval  CrossEntropyLoss |  0.13954348\n","Step    700: eval          Accuracy |  0.95551477\n","\n","Step    800: Ran 100 train steps in 10.90 secs\n","Step    800: train CrossEntropyLoss |  0.11354747\n","Step    800: eval  CrossEntropyLoss |  0.13949209\n","Step    800: eval          Accuracy |  0.95812014\n","\n","Step    900: Ran 100 train steps in 14.74 secs\n","Step    900: train CrossEntropyLoss |  0.11142836\n","Step    900: eval  CrossEntropyLoss |  0.13101284\n","Step    900: eval          Accuracy |  0.95903839\n","\n","Step   1000: Ran 100 train steps in 11.02 secs\n","Step   1000: train CrossEntropyLoss |  0.10680404\n","Step   1000: eval  CrossEntropyLoss |  0.14581710\n","Step   1000: eval          Accuracy |  0.95599332\n"]}]},{"cell_type":"markdown","metadata":{"id":"wMvzQhz9A5Jw"},"source":["##Model Evaluation (using test set)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OzS0RelBz_eD"},"source":["model = NER()\n","model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n","model.init_from_file('model/model.pkl.gz', weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmaJDKW2m_6X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631026351519,"user_tz":-360,"elapsed":1122,"user":{"displayName":"Md. Kamrul Hasan Tuhin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGTfHDQAsNbeejgBYTiTFOcb_5rLG_RZrR72nE4g=s64","userId":"09842155178617471284"}},"outputId":"eda5f367-4eb1-46d6-9603-2578b4551cce"},"source":["x, y = next(data_generator(len(test_sen), test_sen, test_labels, vocab['<PAD>']))\n","print(\"input shapes\", x.shape, y.shape)\n","test_pred = model(x)\n","print(f\"test_pred shape: {test_pred.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input shapes (7194, 70) (7194, 70)\n","test_pred shape: (7194, 70, 17)\n"]}]},{"cell_type":"code","metadata":{"id":"Kv3GhwxohRsm"},"source":["def evaluate_prediction(pred, labels, pad):\n","\n","    outputs = np.argmax(pred, axis=2)\n","    print(\"outputs shape:\", outputs.shape)\n","\n","    mask = labels != pad\n","    print(\"mask shape:\", mask.shape)\n","    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq07KxtRhsC0","executionInfo":{"status":"ok","timestamp":1631026617537,"user_tz":-360,"elapsed":16,"user":{"displayName":"Md. Kamrul Hasan Tuhin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGTfHDQAsNbeejgBYTiTFOcb_5rLG_RZrR72nE4g=s64","userId":"09842155178617471284"}},"outputId":"4fbc677a-d9ce-45cc-bd8f-24e7aea94ec7"},"source":["accuracy = evaluate_prediction(test_pred, y, vocab['<PAD>'])\n","print(\"accuracy: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["outputs shape: (7194, 70)\n","mask shape: (7194, 70)\n","accuracy:  0.9593302\n"]}]},{"cell_type":"markdown","metadata":{"id":"vzgAM7NfxHkk"},"source":["## Predicting custom sentence"]},{"cell_type":"code","metadata":{"id":"gj2FDQaCh9JY"},"source":["def predict(sentence, model, vocab, tag_map):\n","    s = [vocab[token] if token in vocab else vocab['<UNK>'] for token in sentence.split(' ')]\n","    batch_data = np.ones((1, len(s)))\n","    batch_data[0][:] = s\n","    sentence = np.array(batch_data).astype(int)\n","    output = model(sentence)\n","    outputs = np.argmax(output, axis=2)\n","    labels = list(tag_map.keys())\n","    pred = []\n","    for i in range(len(outputs[0])):\n","        idx = outputs[0][i] \n","        pred_label = labels[idx]\n","        pred.append(pred_label)\n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6To4ZjEiVj3","executionInfo":{"status":"ok","timestamp":1631024800325,"user_tz":-360,"elapsed":469,"user":{"displayName":"Md. Kamrul Hasan Tuhin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGTfHDQAsNbeejgBYTiTFOcb_5rLG_RZrR72nE4g=s64","userId":"09842155178617471284"}},"outputId":"a2a1f6cd-117a-4aa1-904d-791ff6196af6"},"source":["sentence = \"Hello Tuhin, are you in Bangladesh? Come Dhaka and see how we ride the Rider\"\n","predictions = predict(sentence, model, vocab, tag_map)\n","for x,y in zip(sentence.split(' '), predictions):\n","    if y != 'O':\n","        print(x,y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello B-org\n","Tuhin, I-org\n","Bangladesh? B-geo\n","Dhaka B-geo\n","Rider B-org\n"]}]}]}